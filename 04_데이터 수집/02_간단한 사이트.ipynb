{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b5c875-1ea5-409d-ab17-3d00ba61bdd3",
   "metadata": {},
   "source": [
    "1. 원하는 데이터가 있는 사이트를 찾았는가? ok\n",
    "2. 페이지 소스 보기를 하였을 때 페이지 소스에 원하는 데이터가 있는가? ok\n",
    "3. 주소 분석이 가능한가? ok\n",
    "4. 끝나는 시점을 알 수 있는가? ok\n",
    "\n",
    "-> BS4를 사용하는 것을 권장드립니다. \n",
    "\n",
    "### BS4의 특정\n",
    "- 웹 요청 주소를 이용해 서버에 직접 요청을 할 수 있다.\n",
    "- 내부적으로 웹 서버가 보내주는 데이터를 받아다 사용하기 때문에 속도가 selenium에 비해서 빠르다.\n",
    "- 하지만 DOM 코드를 가져다 사용할 수는 없다.\n",
    "- 웹 문서에서 소스보기를 했을 때 보이는 코드(즉 서버에서 받아온 코드)를 분석할 때 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb38b356-8af7-4a08-b929-8cc3fd622c80",
   "metadata": {},
   "source": [
    "### BS4를 통해 데이터를 수집하는 코드 작성 순서\n",
    "1. 첫 번째 페이지에서 원하는 데이터를 가져올 수 있도록 구현한다.\n",
    "2. 다음 페이지로 넘어갈 수 있도록 구현한다.\n",
    "3. 마지막 페이지 여부를 확인하여 중단시킬 수 있도록 구현한다.\n",
    "4. 모든 구현이 끝나면 몇 페이지 돌려보세요.\n",
    "5. 반드시 요청하는 페이지의 주소를 파일에 기록하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d34f1b3-7755-48ef-8cf1-479806ed6933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가져온 html 데이터 분석을 위한 라이브러리\n",
    "import bs4\n",
    "# 웹 서버에 요청을 하기 위한 라이브러리\n",
    "import requests\n",
    "# 데이터 분석 라이브러리; 수집한 데이터를 저장하기 위해 사용하겠습니다\n",
    "import pandas as pd\n",
    "# 행렬 및 선형대수학을 위한 라이브러리; 결측치 값을 사용하기 위해 사용\n",
    "import numpy as np\n",
    "# 딜레이를 위해..\n",
    "import time\n",
    "# 컴퓨터나 os와 관련된 라이브러리; 파일 존재 여부를 확인하기 위해 사용\n",
    "import os\n",
    "# 주피터 노트북에서 출력된 내용을 지우기 위해 사용함\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "850877d3-1bad-4d2b-85bb-83c0419cb3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요청함수\n",
    "def getSource(site) :\n",
    "    \"\"\"\n",
    "        특정 웹 사이트에 접속하여 bs4 객체를 생성해 반환한다.\n",
    "\n",
    "        site \n",
    "            요청할 페이지의 주소\n",
    "\n",
    "        return 값\n",
    "            html 소스가 담겨져 있는 bs4 객체\n",
    "    \"\"\"\n",
    "\n",
    "    # 해더 정보 설정\n",
    "    # user-agent : 웹 브라우저가 서버로 보내는 문자열이고 서버는 이를 통해 브라우저 정보나 컴퓨터 정보를 파악한다.\n",
    "    # 일부 사이트에는 user-agent가 전달되지 않으면 데이터를 전달하지 않는 경우도 있다.\n",
    "    # 이에 user-agent를 셋팅하여 요청한 도구가 python 코드를 통한 것이 아닌 일반 웹브라우저를 통해 요청한 것처럼 속일 수 있다.\n",
    "    # https://m.avalon.co.kr/check.html 에서 user-agent 확인이 가능함. \n",
    "    header_info = {\n",
    "        'User-Agent' : 'Mozilla/5.0(Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    # 요청한다.\n",
    "    response = requests.get(site, headers=header_info)\n",
    "    # bs4 객체를 생성한다\n",
    "    soup = bs4.BeautifulSoup(response.text, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d00beb9-0792-4346-b700-f2ef78d0c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 페이지의 데이터를 수집해 저장하는 함수\n",
    "def getData(soup, file_name) : \n",
    "    \"\"\"\n",
    "        한 페이지의 데이터를 수집해 파일에 저장하는 함수\n",
    "\n",
    "        soup\n",
    "            HTML 데이터를 관리하는 bs4 객체\n",
    "\n",
    "        file_name\n",
    "            데이터를 저장할 파일 이름\n",
    "    \"\"\"\n",
    "\n",
    "    # 데이터가 있는 전체를 가져온다.\n",
    "    a1 = soup.select_one('body > main > div > div')\n",
    "    # print(a1)\n",
    "\n",
    "    # 수집한 데이터를 담아 저장하는 용도로 사용할\n",
    "    # 데이터 프레임을 생성하기 위해 사용할 딕셔너리\n",
    "    data_dict = {\n",
    "        '큰제목' : [],\n",
    "        '작은제목' : [],\n",
    "        '날짜' : [],\n",
    "        '작성자' : [],\n",
    "    }\n",
    "\n",
    "    # 내부의 div 태그들을 가져온다. \n",
    "    a2 = a1.select('div')[:-1]\n",
    "    for div in a2 :\n",
    "       # print(div)\n",
    "        # 큰 제목을 가져온다.\n",
    "        a3 = div.select_one('h3 > a')\n",
    "        data1 = a3.text.strip()\n",
    "        if len(data1) == 0:\n",
    "            data1 = np.nan #길이가 0이라면 결측치에 해당하는 값을 가져온다\n",
    "        # print(f'data1 : {data1}')\n",
    "        # print('----------------------')\n",
    "        \n",
    "        # 작은 제목을 가져온다.\n",
    "        a4 = div.select_one('h4')\n",
    "        data2 = a4.text.strip()\n",
    "        if len(data2) == 0:\n",
    "            data2 = np.nan\n",
    "        # print(f'data2 : {data2}')\n",
    "        # print('----------------------')\n",
    "\n",
    "        # 날짜와 작성자\n",
    "        a5 = div.select_one('p > span')\n",
    "        a6 = a5.text.split('|')\n",
    "        data3 = a6[0].strip()\n",
    "        if len(data3) == 0:\n",
    "            data3 = np.nan\n",
    "        data4 = a6[1].strip()\n",
    "        if len(data4) == 0:\n",
    "            data4 = np.nan\n",
    "        print(f'data3 : {data3}')\n",
    "        print(f'data4 : {data4}')\n",
    "\n",
    "        # 딕셔너리에 데이터를 담는다.\n",
    "        data_dict['큰제목'].append(data1)\n",
    "        data_dict['작은제목'].append(data2)\n",
    "        data_dict['날짜'].append(data3)\n",
    "        data_dict['작성자'].append(data4)\n",
    "\n",
    "    # 데이터 프레임을 생성한다.\n",
    "    df1 = pd.DataFrame(data_dict)\n",
    "    # display(df1)\n",
    "\n",
    "    # 저장한다.\n",
    "    # 만약 파일이 없다면\n",
    "    if os.path.exists(file_name) == False :\n",
    "        df1.to_csv(file_name, encoding='utf-8-sig', index=False)\n",
    "\n",
    "    # 만약 파일이 있다면\n",
    "    else : \n",
    "        df1.to_csv(file_name, encoding='utf-8-sig', index=False, header=None, mode='a')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7b6637b-19cb-4783-b96b-2b0425b27ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음 페이지 주소를 가져오는 함수\n",
    "def getNextPage(soup) :\n",
    "    \"\"\"\n",
    "        다음 페이지 주소를 가져오는 함수\n",
    "\n",
    "        soup \n",
    "            HTML 데이터를 가지고 있는  bs4 객체\n",
    "\n",
    "        반환값\n",
    "            다음 페이지가 없다면 None을 반환한다.\n",
    "            다음 페이지가 있다면 요청할 주소에 붙힐 다음 페이지에 대한 정보를 반환한다\n",
    "    \"\"\"\n",
    "    # Next 버튼의 태그를 가져온다.\n",
    "    next_tag = soup.select_one('body > main > div > div > div.pagination > ul > li:nth-child(6) > a')\n",
    "    # print(next_tag)    \n",
    "    # 가져온 것이 있다면\n",
    "    if next_tag != None :\n",
    "        # href 속성의 값을 가져온다.\n",
    "        href = next_tag.attrs['href']\n",
    "        return href\n",
    "    else :\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cf4e55f-4121-479c-822d-66a9d9baa9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pjt3591oo.github.io//page4/ 수집중.. \n",
      "data3 : 2017-04-04 13:10:05 +0000\n",
      "data4 : 박정태\n",
      "수집완료\n"
     ]
    }
   ],
   "source": [
    "# 요청할 페이지의 주소 (첫 페이지 주소)\n",
    "# 어떤 페이지인지를 나타내는 값이 있으면 지워주세요\n",
    "site = 'https://pjt3591oo.github.io/'\n",
    "# 수집을 하고자 하는 페이지를 나타내는 값\n",
    "page = ''\n",
    "\n",
    "# 반복한다.\n",
    "while True : \n",
    "    # 딜레이\n",
    "    time.sleep(1)\n",
    "\n",
    "    # 기존에 출력된 것을 청소한다.\n",
    "    clear_output(wait = True)\n",
    "\n",
    "    print(f'{site}{page} 수집중.. ')\n",
    "    # 현재 수집하고자 하는 사이트의 주소를 파일에 기록한다.\n",
    "    # r : 읽기, w : 기존 내용 지우고 쓰기, a : 기존 내용에 이어서 쓰기\n",
    "    with open('02_log.txt', 'a', encoding='utf-8') as fp : \n",
    "        fp.write(f'{site}{page}\\n')\n",
    "\n",
    "    # 페이지 요청\n",
    "    soup = getSource(site + page)\n",
    "    # 현재 페이지에서 데이터를 가져와 저장한다.\n",
    "    getData(soup, 'data1.csv')\n",
    "    # 다음 페이지의 정보를 가져온다.\n",
    "    page = getNextPage(soup)\n",
    "\n",
    "    # 만약 다음 페이지가 없다면 중단한다.\n",
    "    if page == None :\n",
    "        print('수집완료')\n",
    "        break\n",
    "\n",
    "\n",
    "#soup = getSource('https://pjt3591oo.github.io/page4/')\n",
    "#print(getNextPage(soup))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
